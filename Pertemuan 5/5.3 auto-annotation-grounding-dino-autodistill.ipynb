{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WE2JHng4IBC0"
      },
      "source": [
        "# Auto Annotation using Grounding DINO + Autodistill\n",
        "\n",
        "- Autodistill :\n",
        "    - Autodistill uses *big, slower foundation models* to train small, faster supervised models. \n",
        "    - Using autodistill, you can go from *unlabeled images* to inference with no human intervention in between.\n",
        "    - As foundation models get better and better they will increasingly be able to augment or replace humans in the labeling process. \n",
        "    <img src=\"resource/autodistil.jpg\" width=\"800px\">\n",
        "    - More about Autodistill Usage (https://docs.autodistill.com/quickstart/#step-5-label-a-dataset)\n",
        "- Grounding DINO : \n",
        "    - Grounding DINO is a `self-supervised learning` algorithm that combines **DINO** (*DETR with Improved deNoising anchor boxes*) with **GLIP** (*Grounded Language-Image Pre-training*). \n",
        "    - Grounding DINO linking textual descriptions (*performed by GLIP*) to their respective visual representations (*performed by DINO*).\n",
        "    - Grounding DINO can detect `arbitrary objects` with `human inputs` such as `category names` or `referring expressions`. \n",
        "    <img src=\"resource/grounding-dino.png\" width=\"800px\"><br><br>\n",
        "    <img src=\"resource/model_explan1.png\" width=\"800px\"><br><br>\n",
        "    <img src=\"resource/model_explan2.png\" width=\"800px\"><br><br>\n",
        "    - More about Grounding DINO (https://github.com/IDEA-Research/GroundingDINO)\n",
        "    - DINO Paper (https://arxiv.org/pdf/2203.03605)\n",
        "    - GLIP Paper (https://arxiv.org/pdf/2112.03857)\n",
        "    - Grounding DINO Paper (https://arxiv.org/abs/2303.05499)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What we will do\n",
        "- Setting GPU Environment\n",
        "- Installing Autodistill & Grounding DINO\n",
        "- Download Dataset from Google Drive Shared Link (`Dataset Scissors.zip`)\n",
        "- Perform Automatic Annotation using Autodistill & Grounding DINO\n",
        "- Upload Dataset & Annotation to Roboflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "⚠️⚠️⚠️ *Please open this notebook in Google Colab* by click below link ⚠️⚠️⚠️<br><br>\n",
        "<a href=\"https://colab.research.google.com/github/Muhammad-Yunus/Belajar-OpenCV-ObjectDetection/blob/main/Pertemuan%205/5.3 auto-annotation-grounding-dino-autodistill.ipynb\" target=\"_blank\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 Connect GPU Environment \n",
        "\n",
        "- Click `Connect` button in top right Google Colab notebook,<br>\n",
        "<img src=\"resource/cl-connect-gpu.png\" width=\"250px\">\n",
        "- If connecting process completed, it will turn to something look like this<br>\n",
        "<img src=\"resource/cl-connect-gpu-success.png\" width=\"250px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Check GPU connected into Colab environment is active"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uLRz7KlO2eMv"
      },
      "outputs": [],
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "\n",
        "import os\n",
        "HOME = os.getcwd()\n",
        "print(\"HOME:\", HOME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.2 Installing Autodistill & Grounding DINO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-zp_iQUiIflc"
      },
      "outputs": [],
      "source": [
        "!pip install roboflow autodistill autodistill-grounding-dino supervision -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOQ5RSYTKDBy"
      },
      "source": [
        "## 3.3 Download Dataset from Google Drive\n",
        "- We will use sample dataset [Dataset Scissors.zip](https://drive.google.com/file/d/1PFUlHCwxHAz-zMpP7UD-L47dRAzbrqWi/view) from GDrive.\n",
        "- If you want to use **your own dataset**, just upload dataset to GDrive and share as **public link** in **ZIP** format.<br>\n",
        "<img src=\"resource/gd-share.png\" width=\"400px\">\n",
        "- Open the shared link in browser, and copy the `GDrive ID` in browser address bar.<br>\n",
        "<img src=\"resource/gd-id.png\" width=\"600px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Paste the GDrive ID as value `gdrive_id` variable below,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53tUtWm5FsMC"
      },
      "outputs": [],
      "source": [
        "!pip install gdown\n",
        "\n",
        "import gdown\n",
        "\n",
        "DATASET_NAME = 'Scissors_Dataset'\n",
        "\n",
        "# default using gdrive_id Dataset `Scissors.zip` (1PFUlHCwxHAz-zMpP7UD-L47dRAzbrqWi)\n",
        "gdrive_id = '1PFUlHCwxHAz-zMpP7UD-L47dRAzbrqWi' # <-----  ⚠️⚠️⚠️ USE YOUR OWN GDrive ID ⚠️⚠️⚠️\n",
        "\n",
        "# download zip from GDrive\n",
        "url = 'https://drive.google.com/uc?id=1PFUlHCwxHAz-zMpP7UD-L47dRAzbrqWi'\n",
        "gdown.download(url, DATASET_NAME + \".zip\", quiet=False)\n",
        "\n",
        "# unzip dataset\n",
        "!unzip -j {DATASET_NAME}.zip -d {DATASET_NAME}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bjXknigMgqF"
      },
      "source": [
        "## 3.4 Automatic Annotation using Autodistill & Grounding DINO\n",
        "\n",
        "- Below, we import the dependencies required to running Autodistill & Grounding DINO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3X_nnrUIb-Z"
      },
      "outputs": [],
      "source": [
        "from autodistill_grounding_dino import GroundingDINO\n",
        "from autodistill.detection import CaptionOntology\n",
        "import supervision as sv\n",
        "import cv2\n",
        "import os\n",
        "import torch\n",
        "import random\n",
        "\n",
        "torch.use_deterministic_algorithms(False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UW_YGLv1Mm3Q"
      },
      "source": [
        "#### 3.4.1 Label Images with Grounding DINO\n",
        "\n",
        "- Below, we set an *ontology*. Ontologies map `prompts` -- *text given to a model for use in labelling data* -- to the `labels` you want your dataset to include.\n",
        "- For example, the ontology `\"red color apple\": \"apple\"` will send the prompt `\"red color apple\"` to the base model (in this example Grounding DINO). All objects matching that prompt will be labelled `\"apple\"`.\n",
        "- Example if we have multiple object to detect (`apple`, `bottle`, `cap`): \n",
        "    ```\n",
        "    {\n",
        "        \"red color apple\": \"apple\"\n",
        "        \"milk bottle\": \"bottle\",\n",
        "        \"blue cap\": \"cap\"\n",
        "    }\n",
        "    ```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3VXD6dU5J3oR"
      },
      "outputs": [],
      "source": [
        "DATASET_FOLDER = \"./\" + DATASET_NAME\n",
        "\n",
        "# create ontology for `Dataset_Scissors`\n",
        "# since we only want to detect `scissors` in dataset,\n",
        "# just define prompt `scissors` and label `scissors`\n",
        "#\n",
        "ONTOLOGY = {\n",
        "    \"scissors\": \"scissors\"\n",
        "}\n",
        "\n",
        "# instantiate base_model using Grounding_DINO\n",
        "base_model = GroundingDINO(ontology=CaptionOntology(ONTOLOGY))\n",
        "\n",
        "images = {}\n",
        "annotations = {}\n",
        "\n",
        "image_names = os.listdir(DATASET_FOLDER)\n",
        "\n",
        "for image_name in image_names:\n",
        "\n",
        "  # find full path of the image /path/to/file.jpg\n",
        "  image_name = os.path.join(DATASET_FOLDER, image_name)\n",
        "\n",
        "  # filter only image data in dataset folder\n",
        "  if not image_name.endswith((\".jpg\", \".jpeg\", \".png\")):\n",
        "    print(image_name)\n",
        "    continue\n",
        "\n",
        "  # perform automatic annotation (detect bounding box & label) from image in dataset folder\n",
        "  predictions = base_model.predict(image_name)\n",
        "\n",
        "  # filter only predistion results with confidence > 0.5 (50%)\n",
        "  # you can change the threshold to other number preferred, e.g (0.75 for confidence above 75%)\n",
        "  predictions = predictions[predictions.confidence > 0.5]\n",
        "\n",
        "  # read image using OpenCV\n",
        "  image = cv2.imread(image_name)\n",
        "\n",
        "  # save the result into `annotations` and `images` dictionary\n",
        "  annotations[image_name] = predictions\n",
        "  images[image_name] = image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrCTCytRK1RG"
      },
      "source": [
        "- Check detected boundingbox result  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h0jaTjNV6bhp"
      },
      "outputs": [],
      "source": [
        "for image_name in image_names[:2]: # [:2] --> get only 2 sample image for checking the box result\n",
        "  image_name = os.path.join(DATASET_FOLDER, image_name)\n",
        "\n",
        "  image = images[image_name]\n",
        "  detections = annotations[image_name]\n",
        "\n",
        "  box_annotator = sv.BoxAnnotator(thickness=20)\n",
        "  annotated_image = box_annotator.annotate(scene=image.copy(), detections=detections)\n",
        "\n",
        "  %matplotlib inline\n",
        "  sv.plot_image(annotated_image, (5, 5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.5 Upload Generated Annotation and Image Dataset to Roboflow\n",
        "\n",
        "\n",
        "- Convert and save `annotations` data into `Pascal VOC` in `.xml` format\n",
        "- The format is required by Roboflow upload API\n",
        "- More abount `Pascal VOC `Format (http://host.robots.ox.ac.uk/pascal/VOC/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kysgTuR12U-d"
      },
      "outputs": [],
      "source": [
        "ANNOTATIONS_DIRECTORY = os.path.join(HOME, 'annotations')\n",
        "\n",
        "MIN_IMAGE_AREA_PERCENTAGE = 0.002\n",
        "MAX_IMAGE_AREA_PERCENTAGE = 0.80\n",
        "APPROXIMATION_PERCENTAGE = 0.75\n",
        "\n",
        "# extract class names from ONTOLOGY defined on section 3.4.1 above\n",
        "CLASSES = list(ONTOLOGY.values())\n",
        "\n",
        "sv.DetectionDataset(\n",
        "    classes=CLASSES,\n",
        "    images=images,\n",
        "    annotations=annotations\n",
        ").as_pascal_voc(\n",
        "    annotations_directory_path=ANNOTATIONS_DIRECTORY,\n",
        "    min_image_area_percentage=MIN_IMAGE_AREA_PERCENTAGE,\n",
        "    max_image_area_percentage=MAX_IMAGE_AREA_PERCENTAGE,\n",
        "    approximation_percentage=APPROXIMATION_PERCENTAGE\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Create **Roboflow Project** automatically using bellow script\n",
        "- The upload proceess will prompting you with `Roboflow Auth Token`,<br>\n",
        "<img src=\"resource/rb_login.png\" width=\"500px\"><br><br>\n",
        "<img src=\"resource/rb-copy-token.png\" width=\"800px\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KVvMhu5R_B-7"
      },
      "outputs": [],
      "source": [
        "import roboflow\n",
        "from roboflow import Roboflow\n",
        "\n",
        "# define new project name in Roboflow\n",
        "PROJECT_NAME = \"scissors-auto-annotate-1\"\n",
        "PROJECT_DESCRIPTION = \"scissors-auto-annotate-1\"\n",
        "\n",
        "\n",
        "roboflow.login()\n",
        "\n",
        "workspace = Roboflow().workspace()\n",
        "new_project = workspace.create_project(\n",
        "    project_name=PROJECT_NAME,\n",
        "    project_license=\"MIT\",\n",
        "    project_type=\"object-detection\",\n",
        "    annotation=PROJECT_DESCRIPTION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Upload `Image Dataset` (.jpg) and `Pascal VOC` (.xml) to newly created Roboflow Project above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0EnrJVmG_S_q"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "image_paths = sv.list_files_with_extensions(directory=DATASET_FOLDER, extensions=[\"jpg\", \"jpeg\", \"png\"])\n",
        "for image_path in tqdm(image_paths):\n",
        "    image_name = image_path.name\n",
        "    annotation_name = f\"{image_path.stem}.xml\"\n",
        "    image_path = str(image_path)\n",
        "    annotation_path = os.path.join(ANNOTATIONS_DIRECTORY, annotation_name)\n",
        "    new_project.upload(\n",
        "        image_path=image_path,\n",
        "        annotation_path=annotation_path,\n",
        "        split=\"train\",\n",
        "        is_prediction=True,\n",
        "        overwrite=True,\n",
        "        tag_names=[\"scissors-auto-annotate\"],\n",
        "        batch_name=\"scissors-auto-annotate\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Check the uploaded dataset in Roboflow \n",
        "- Open Tab `Annotate`, click on unassigned dataset below<br>\n",
        "<img src=\"resource/rb-auto-uploaded.png\" width=\"800px\">\n",
        "- It will showing all uploaded image data with bounding box\n",
        "- Just click on `Anotate Images` button in top right corner<br>\n",
        "<img src=\"resource/rb-auto-annotated.png\" width=\"800px\">\n",
        "- After that, just click `Manual Labeling` sidebar then click `Assign to Myself`<br>\n",
        "<img src=\"resource/rb-manual-labeling.png\" width=\"300px\">\n",
        "    - *Don't worrie, all data is already annotated, so nolonger need to manually annotated the data*<br><br>\n",
        "- Click on `Add xx images to Dataset` button in the top right corner<br>\n",
        "<img src=\"resource/rb-add-dataset.png\" width=\"800px\">\n",
        "- Then choose method `Split Images Between Train/Valid/Test` and click `Add images` button <br>\n",
        "<img src=\"resource/rb-train-test-split.png\" width=\"300px\">\n",
        "- Completing the process like in `Pertemuan 4` to add `Preprocessing` and `Augmentation` on dataset until the `Create` process.\n",
        "<img src=\"resource/rb-finalized.png\" width=\"600px\">\n",
        "- Now the dataset ready to use, just click on `Download Dataset` button then choose the format to use like usual<br>\n",
        "<img src=\"resource/rb-export.png\" width=\"800px\"><br><br>\n",
        "- Use that Roboflow Dataset in notebook `5.1 train-yolov8-object-detection-on-custom-dataset.ipynb` by your self<br>\n",
        "<a href=\"https://colab.research.google.com/github/Muhammad-Yunus/Belajar-OpenCV-ObjectDetection/blob/main/Pertemuan%205/5.1 train-yolov8-object-detection-on-custom-dataset.ipynb\" target=\"_blank\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open `5.1 train-yolov8-object-detection-on-custom-dataset.ipynb` In Colab\"/></a>\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
